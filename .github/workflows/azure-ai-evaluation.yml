name: Azure AI Evaluation

on:
  # push:
  #   branches: [ main, develop ]
  # pull_request:
  #   branches: [ main ]
  workflow_dispatch:  # Allow manual trigger
  workflow_call:
  # schedule:
  #   - cron: '0 2 * * *'  # Run daily at 2 AM UTC

env:
  PYTHON_VERSION: '3.12'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    env:
      LOG_LEVEL: INFO
      DEBUG_MODE: false
    permissions:
      id-token: write  # Required for Azure OIDC authentication
      contents: read
      issues: write    # Optional: for posting results as comments
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Azure CLI Login (OIDC)
      uses: azure/login@v1
      with:
        client-id: ${{ secrets.AZURE_CLIENT_ID }}
        tenant-id: ${{ secrets.AZURE_TENANT_ID }}
        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
    
    - name: Run Azure AI Evaluation
      id: evaluation
      env:
        AZURE_AI_PROJECT_ENDPOINT: ${{ secrets.AZURE_AI_PROJECT_ENDPOINT }}
        AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
        AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
        AZURE_OPENAI_CHAT_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_CHAT_DEPLOYMENT }}
        AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
        EVAL_DATA_PATH: ../data/eval_data.jsonl
        OUTPUT_PATH: ../results/evaluation_results_${{ github.run_number }}.json
        EVALUATION_THRESHOLD: 3
        LOG_LEVEL: INFO
        DEBUG_MODE: false
      run: |
        cd apps
        mkdir -p ../results
        
        # Run evaluation and capture studio URL
        python 01_azure_ai_evaluation_batch.py 2>&1 | tee evaluation_output.log
        
        # Try to extract studio URL using multiple methods to avoid masking
        STUDIO_URL_ENCODED=$(grep "GITHUB_ACTIONS_STUDIO_URL_ENCODED=" evaluation_output.log | cut -d'=' -f2- || echo "")
        STUDIO_URL_DIRECT=$(grep "AZURE_AI_STUDIO_LINK=" evaluation_output.log | cut -d'=' -f2- || echo "")
        
        # Use the encoded URL and decode it, or fall back to direct URL
        if [ -n "$STUDIO_URL_ENCODED" ]; then
          # URL decode the encoded URL
          STUDIO_URL=$(python3 -c "import urllib.parse; print(urllib.parse.unquote('$STUDIO_URL_ENCODED'))")
          echo "studio_url=$STUDIO_URL" >> $GITHUB_OUTPUT
        elif [ -n "$STUDIO_URL_DIRECT" ]; then
          echo "studio_url=$STUDIO_URL_DIRECT" >> $GITHUB_OUTPUT
        fi
        
        # Also save to a file to avoid masking issues
        if [ -n "$STUDIO_URL" ] || [ -n "$STUDIO_URL_DIRECT" ]; then
          FINAL_URL="${STUDIO_URL:-$STUDIO_URL_DIRECT}"
          echo "$FINAL_URL" > ../results/studio_url.txt
        fi
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v4
      if: always()  # Upload even if evaluation fails
      with:
        name: evaluation-results-${{ github.run_number }}
        path: |
          results/
          apps/evaluation.log
        retention-days: 30
    
    - name: Parse and display results
      if: success()
      run: |
        RESULT_FILE="results/evaluation_results_${{ github.run_number }}.json"
        STUDIO_URL="${{ steps.evaluation.outputs.studio_url }}"
        
        # Try to get studio URL from file if not available from output
        if [ -z "$STUDIO_URL" ] && [ -f "results/studio_url.txt" ]; then
          STUDIO_URL=$(cat results/studio_url.txt)
        fi
        
        echo "## Evaluation Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Add Azure AI Foundry link if available
        if [ -n "$STUDIO_URL" ]; then
          echo "ðŸ”— **[View detailed results in Azure AI Foundry]($STUDIO_URL)**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "### Summary" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "$RESULT_FILE" ]; then
          # Use jq to parse JSON and create markdown table
          if command -v jq &> /dev/null; then
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            jq -r '.metrics | to_entries[] | "| \(.key) | \(.value) |"' "$RESULT_FILE" >> $GITHUB_STEP_SUMMARY
          else
            # Fallback: just show the file exists
            echo "Evaluation completed. Results saved to $RESULT_FILE" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "No results file found at $RESULT_FILE" >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Comment on PR with results
      if: github.event_name == 'pull_request' && success()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = `results/evaluation_results_${{ github.run_number }}.json`;
          let studioUrl = '${{ steps.evaluation.outputs.studio_url }}';
          
          // Try to get studio URL from file if not available from output
          if (!studioUrl && fs.existsSync('results/studio_url.txt')) {
            studioUrl = fs.readFileSync('results/studio_url.txt', 'utf8').trim();
          }
          
          if (fs.existsSync(path)) {
            const results = JSON.parse(fs.readFileSync(path, 'utf8'));
            
            let comment = '## ðŸ¤– Azure AI Evaluation Results\n\n';
            
            // Add Azure AI Foundry link if available
            if (studioUrl) {
              comment += `ðŸ”— **[View detailed results in Azure AI Foundry](${studioUrl})**\n\n`;
            }
            
            if (results.metrics) {
              comment += '### Metrics\n\n';
              comment += '| Metric | Value |\n';
              comment += '|--------|-------|\n';
              
              for (const [key, value] of Object.entries(results.metrics)) {
                const displayValue = typeof value === 'number' ? value.toFixed(4) : value;
                comment += `| ${key} | ${displayValue} |\n`;
              }
              comment += '\n';
            }
            
            if (results.rows) {
              comment += `### Summary\n- Evaluated ${results.rows.length} test cases\n`;
            }
            
            comment += `\n*Evaluation completed at ${new Date().toISOString()}*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }
